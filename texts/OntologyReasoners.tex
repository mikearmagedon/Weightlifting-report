
This section provides an overview and comparison of two reasoners, Pellet and Hermit, taking into account the final application and ontology. The Fact++ and RacePro reasoners are also included in the comparison to provide an additional reference for comparison.
Although the target ontology's characteristics are not yet known, the analysis performed on the specified reasoners is based on the work performed by \cite{Dentler2011} and \cite{Bock2008}, which consist on the creation of complete benchmarks based on several important characteristics, such as practicability and performance.

A reasoner is a program that infers logical consequences from a list of explicit asserted facts or axioms and typically provides automated support for reasoning tasks such as classification, debugging and querying.

The reasoner presented are: HermiT \cite{1_hermitreasoner}, Pellet \cite{2_stardog-union/pellet_2016}, Fact++ \cite{3_fact++reasoner} and RacerPro \cite{4_racerpro_2016}. This set has several common characteristics which seem to be essential to the final application. In terms of practical usability, they are accessible through the OWL API and can be plugged into Protege, except RacerPro. Since they possess the same interfaces, they can be interchanged providing development flexibility, allowing to easily compare them. Also all the referred reasoners implement tableux algorithms which guarantees soundness - all the inferred statements a correct - and completeness - how many correct statements are inferred. A small description of each reasoner follows:

\begin{itemize}
\item \textbf{Hermit} - This Java-based OWL reasoner is based on a new tableau reasoning algorithm. It can be integrated into Protege e and Java applications using the OWL API.
\item \textbf{Pellet} - This is an open-source, Java-based OWL DL reasoning engine that supports a majority of the constructs of OWL, including those introduced in OWL 2. Pellet is developed and commercially supported by Clark and Parsia.
\item \textbf{FaCT++} - An open-source reasoner, which implemented in C++, supports large subset of OWL DL and is based on optimized tableaux algorithms.
\item \textbf{RacerPro} (Renamed ABox and Concept Expression
Reasoner, Racer Systems) supports a large subset of OWL DL. It is implemented in the Common Lisp programming language.
\end{itemize}

\subsubsection{Performance Comparison}

The developed performance benchmark in \cite{Dentler2011} uses two typical reasoner tasks, consistency checking and classification. Consistency checking verifies whether there exists a relational structure that satisfies all axioms in the given ontology, that is, if there are no contradictory statements in the ontology. Classification is the computation of the concept hierarchy and relationship. This is often used as the main performance indicator. The ontology should be checked for consistency and classified regularly, with every modification made to it. 

Using several well established ontologies in the biomedical field, \cite{Dentler2011} evaluates the performance of several reasoners (including HermiT, Pellet and Fact++) according to the previously mentioned reasoner tasks, among other characteristics. This ontologies have the same OWL profile but different sizes in order to evaluate a possible trade off between ontologies' complexity and reasoner's performance. The ontologies used were GO, NCI and SNOMED CT which are listed in ascendant order of size in tables \ref{tab:1} and \ref{tab:2}. According to the data provided in \cite{Dentler2011} present in table \ref{tab:1}, all tested reasoners succeeded in classifying SNOMED CT and the Fact++ was the only reasoner who classified the NCI faster than the GO ontology. HermiT reasoner had the worst performance when classifying the SNOMED CT ontology taking up to two hours to perform the task.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
      &  HermiT    &  Pellet     &  Fact++    \\ \hline
GO    &    6.48    &  3.41       &  20.75     \\ \hline
NCI   &   11.75    &  14.84      &  11.10     \\ \hline
S CT  &  6,793.76  &  1,345.65   &  700.87    \\ \hline
\end{tabular}
\caption{Reasoner classification performance comparison in \cite{Dentler2011} (s).}
\label{tab:1}
\end{table}

Consistency checking performance per reasoner is depicted in table \ref{tab:1}. The recorded times are mostly insignificant except for the SNOMED CT ontology where HermiT outperforms the other reasoners by a large margin.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
      &  HermiT  &  Pellet   &  Fact++ \\ \hline
GO    &    0.00  &  0.27     &  0.36   \\ \hline
NCI   &    0.00  &  0.38     &  0.71   \\ \hline
S CT  &    0.00  &  16.78    &  15.3   \\ \hline
\end{tabular}
\caption{Reasoner consistency check performance comparison in \cite{Dentler2011} (s).}
\label{tab:2}
\end{table}

It should be noted that, independently of the used reasoner, the performance decreases with increasing ontology size and complexity.

In \cite{Bock2008} a different kind of analysis is performed. Reasoners are compared according to their Load and Response Time.
Load Time is the time required to perform some important preparation before classification, consistency check and querying ontologies. Response Time starts with querying execution and ends when all the results are stored in a local variable. 
In the tests by \cite{Dentler2011}, the tested ontologies were implemented according to the same OWL profile. The study by \cite{Bock2008} takes a diffrent approach and tests ontologies implemented in different OWL profiles, which appear in ascending expressiveness order in tables \ref{tab:3} and \ref{tab:4}. 

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
          &   HermiT  &  Pellet & RacerPro\\ \hline
VICODI    &    0.889  &  0.563  & 0.110   \\ \hline
SWRC      &    0.776  &  0.518  & 0.092   \\ \hline
LUBM      &    0.708  &  0.404  & 0.072   \\ \hline
WINE      &    1.090  &  0.835  & 0.0168  \\ \hline
\end{tabular}
\caption{Load time performance comparison in \cite{Bock2008} (s).}
\label{tab:3}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
          &  HermiT  &  Pellet  & RacerPro \\ \hline
VICODI    &    0.180  &  1.145  & 0.080 \\ \hline
SWRC      &    0.046  &  0.346  & 0.056 \\ \hline
LUBM      &    0.046  &  0.253  & 0.365 \\ \hline
WINE      &    0.465  &  9.252  & 0.607 \\ \hline
\end{tabular}
  \caption{Response time performance comparison in \cite{Bock2008} (s).}
\label{tab:4}
\end{table}

By looking at the above tables it is clear that, although Pellet and RacePro show better performance at load time, they also shows significant less performance than HermiT when queries are executed, especially for higher complexity ontologies. This comparison depends on whether the application frequently operates on preloaded ontologies or has to re-loaded them at each classification, since in the former case load times become irrelevant. 


\subsubsection{Final Thoughts}

Based on the literature, two benchmarks and the respective results were analyzed to select the reasoners that best fit application's requirements. Between Pellet and HermiT, at first sight the HermiT reasoner shows a better performance than Pellet.  Unfortunately, a decision can not be made on a specific reasoner since application requirements are not yet known. Another important point for reasoner selection are the target ontology's characteristics, such as size or expressiveness, which are also unknown. 

Since reasoner performance is largely dependent on ontology and application characteristics, when developing an ontology and its application, the various reasoners should be tested regularly in order to decide which one bests suits the system requirements. Because of this, an important characteristic of the selected reasoners is that they should provide an interface via a common ontology API, such as the OWL API, and have a Protege plugin, which will allow to easily interchange them at development time for performance testing. 
